{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TypDOiTPbjE"
   },
   "source": [
    "# LangChain 기능 소개\n",
    "\n",
    "- 1. 프롬프트 템플릿과 로더를 사용하여 체인 구성  \n",
    "- 2. Runnable과 LangChain 표현 언어  \n",
    "- 3. RAG 체인 구축  \n",
    "- 4. 체인이 달린 도구 사용  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOUiSLMxPzZY"
   },
   "source": [
    "## LangChain 개요\n",
    "\n",
    "LangChain으로 개발된 애플리케이션은 일반적으로 세 가지 범주로 나뉩니다.\n",
    "\n",
    "- 챗봇 : LLM을 이용하여 보다 지능적인 마케팅, 고객 지원, 교육 상호작용 구현.\n",
    "- 검색 증강 생성(RAG) Q&A : 대용량 문서 요약, 데이터 분석, 외부 소스를 참조하여 코드 생성에 사용.\n",
    "- 에이전트 시스템은 다중 에이전트 설정과 인간 상호작용을 포함하며 복잡한 워크플로우를 위해 LangGraph를 활용. 공급망 관리 및 운영 최적화와 같은 분야에 적용 가능.\n",
    "\n",
    "LangChain은 자연어를 실행 가능한 프로그램으로 변환하는 파이프라인을 생성할 수 있게 합니다. 이러한 체인을 활용함으로써 사용자는 자연어를 입력하고 보고서, 분석 또는 컴퓨터 프로그램과 같은 출력을 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-core langchain\n",
    "# !pip install -U langchain-openai\n",
    "# !pip install -U langchain-google-genai\n",
    "# !pip install -U langchain-community\n",
    "# !pip install -U langchain-experimental\n",
    "# !pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXRwb1YcsW1j",
    "outputId": "7e5cff59-b6f5-446d-e3cc-93891014f5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env 파일에서 API 키를 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용 가능한 LLM 목록 조회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEi1gjmpW8ab"
   },
   "source": [
    "## LLM 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "from openai.error import AuthenticationError\n",
    "\n",
    "# 모델 초기화\n",
    "model = init_chat_model(\"gpt-5-nano\", model_provider=\"openai\")\n",
    "\n",
    "# API Key 유효성 강제 테스트 함수\n",
    "def validate_key(model):\n",
    "    try:\n",
    "        # 아주 짧은 테스트 호출\n",
    "        response = model.invoke([HumanMessage(content=\"ping\")])\n",
    "        print(\"✅ API Key 유효합니다. 테스트 응답:\", response.content)\n",
    "    except AuthenticationError as e:\n",
    "        print(\"❌ API Key 인증 실패:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ 다른 오류 발생:\", e)\n",
    "\n",
    "validate_key(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jSY2uUq8ssCp"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-5-nano\", model_provider=\"openai\")\n",
    "# model = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "NFWMvMRitGX-",
    "outputId": "9eff0799-8467-4610-ee63-d3b945657364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음은 한국어로 정리한 LangChain에 대한 설명입니다.\n",
      "\n",
      "무엇인가요?\n",
      "- LangChain은 대형언어모델(LLM, 예: GPT-4) 기반의 애플리케이션을 쉽게 만들 수 있도록 돕는 프레임워크예요. 데이터 추출, 문서 요약, 대화형 봇, 코드 보조 등 다양한 LLM 기반 워크플로우를 구성하고 관리하기 위한 도구 모음이라고 보면 됩니다.\n",
      "\n",
      "주요 구성 요소\n",
      "- LLM (대형언어모델): 실제로 텍스트를 생성하고 이해하는 모델. OpenAI, Cohere, HuggingFace 등 여러 공급자를 연결해 사용할 수 있어요.\n",
      "- 프롬프트 템플릿(Prompt Templates): 입력 변수(예: topic, user_id 등)를 활용해 재사용 가능한 프롬프트를 미리 정의하는 방식.\n",
      "- 체인(Chains): 여러 단계의 작업 흐름을 순차적으로 수행하는 파이프라인. 예를 들어 프롬프트 만들기 -> LLM 호출 -> 결과 파싱 같은 흐름.\n",
      "- 에이전트(Agents)와 도구(Tools): LLM이 필요에 따라 외부 도구(API)나 데이터에 접근해 실행할 수 있도록 하는 고급 기능. 예: 웹 검색, 계산기, 데이터베이스 질의 등 도구를 사용하도록 지시.\n",
      "- 메모리(Memory): 대화의 맥락이나 상태를 기억해 오래된 내용도 참고할 수 있게 하는 기능. 대화형 봇의 지속성에 유용.\n",
      "- 벡터 저장소(Vector Stores)와 임베딩(Embeddings): 문서나 지식을 벡터로 저장하고 필요 시 유사도 검색으로 정보를 회수하는 컴포넌트. Retrieval-Augmented Generation(RAG) 구현에 자주 씁니다.\n",
      "- 문서 로더/데이터 소스: PDFs, 웹페이지, 파일 시스템 등에서 데이터를 가져와 지식으로 구축하는 도구들.\n",
      "- 저장 및 재현성: 파이프라인 정의를 코드로 기록하고, 재현 가능하게 실행합니다.\n",
      "\n",
      "어떤 흐름으로 작동하나요?\n",
      "- 일반적인 사용 흐름은 다음과 같습니다.\n",
      "  1) 데이터나 지식을 수집/정리(문서 로더, 임베딩 등).\n",
      "  2) 프롬프트 템플릿으로 입력 포맷과 예시를 정의.\n",
      "  3) LLM 체인이나 에이전트를 구성해 필요한 작업 흐름을 만듭니다.\n",
      "  4) 메모리나 벡터 저장소를 추가해 맥락 유지와 정보 검색 가능성 강화.\n",
      "  5) 실행하고 출력에 대한 후처리나 평가를 수행합니다.\n",
      "- 필요에 따라 도구를 호출하거나 외부 서비스와 연동하는 에이전트 방식으로 확장할 수 있습니다.\n",
      "\n",
      "언제 주로 쓰이나요?\n",
      "- 예시 사용 사례\n",
      "  - 고객 상담용 챗봇: 대화 맥락 유지, 문서 기반의 정확한 답변 제공.\n",
      "  - 문서 QA 시스템: 대량의 매뉴얼, 규정서 등을 바탕으로 질문에 대한 구체적 답변 추출.\n",
      "  - 코드 보조 및 자동화: 코드 예시 생성, 리팩토링 제안, API 호출 자동화 등.\n",
      "  - 요약/리포트 제작: 긴 문서를 요약하고 핵심 포인트를 정리.\n",
      "  - 지식 기반과의 상호작용이 필요한 에이전트: 웹 검색, 계산, 데이터 질의 등 여러 도구를 조합해 작업 수행.\n",
      "\n",
      "LangChain의 차이점\n",
      "- Chain vs Agent\n",
      "  - Chains: 정해진 순서대로 작업을 수행하는 단순한 파이프라인. 예를 들어 프롬프트 생성 → LLM 호출 → 결과 정제 같은 흐름.\n",
      "  - Agents: 상황에 따라 동적으로 도구를 선택하고 사용합니다. 필요에 따라 웹 검색, 계산기 사용, 데이터베이스 질의 등을 수행할 수 있어 더 유연한 자동화가 가능합니다.\n",
      "- Memory: 대화의 맥락을 유지하고 반복된 대화에서 일관되게 응답하도록 도와줍니다.\n",
      "- Retrieval + Vector Stores: 필요한 정보를 벡터 검색으로 찾아와 LLM의 응답에 보강하는 기능(RAG)에 많이 사용됩니다.\n",
      "\n",
      "초보자가 시작할 때의 간단한 팁\n",
      "- 설치와 기본 사용:\n",
      "  - 파이썬: pip install langchain\n",
      "  - 자바스크립트/타입스크립트: npm install langchain\n",
      "- 간단한 예시 흐름을 먼저 만들어 보세요. 프롬프트 템플릿 작성 → LLM 체인 구성 → 실행 결과 확인.\n",
      "- 문서와 예제를 통해 API 변화에 주의하세요. LangChain은 Python과 JS/TS 버전이 각각 존재하고, 버전에 따라 API가 조금씩 다를 수 있습니다.\n",
      "- 비용과 속도 관리: LLM 호출은 비용에 직접적인 영향을 주므로 프롬프트 길이, 토큰 수, 비동기 처리 등을 고려해 설계하세요.\n",
      "- 데이터 프라이버시: 외부 API를 호출하거나 외부 데이터 소스를 사용할 때는 데이터 보안과 정책을 확인하세요.\n",
      "\n",
      "간단한 시작용 참고\n",
      "- 공식 문서와 예제에서 본인의 필요에 맞춘 체인/에이전트 구성을 따라 하는 것이 가장 빠릅니다.\n",
      "- 원하시면 특정 용도(예: 문서 QA 봇, 내부 지식 베이스 검색 에이전트 등)에 맞춘 간단한 설계 예시를 함께 구성해 드릴게요.\n",
      "\n",
      "원하는 용도나 현재 수준을 알려주시면, 구체적으로 어떤 구성요소를 먼저 써보면 좋을지, 간단한 예제 설계나 코드 스니펫까지 맞춤으로 도와드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "- AIMessage: AI(인공지능) 모델이 생성한 메시지를 나타냅니다.  \n",
    "예를 들어, AI 모델이 사용자의 질문에 대해 응답을 생성할 때 이 클래스가 사용됩니다.\n",
    "\n",
    "- HumanMessage: 사람(사용자)이 생성한 메시지를 나타냅니다.  \n",
    "사용자가 입력한 텍스트나 질문 등이 여기에 해당됩니다.\n",
    "\n",
    "- StemMessage: AI 행동을 프라이밍하기 위한 메시지.\n",
    "시스템 메시지는 일반적으로 일련의 입력 메시지 중 첫 번째로 전달됩니다. 일반적으로 대화의 흐름을 제어하거나 특정 지침을 제공하기 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='\\n마치 다섯살 먹은 어린아이에게 설명하듯이 한국어로 쉽게 설명해 주세요.\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\nLangChain이 무엇인가요?\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "마치 다섯살 먹은 어린아이에게 설명하듯이 한국어로 쉽게 설명해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "LangChain이 무엇인가요?\n",
    "\"\"\"\n",
    "\n",
    "# system과 human/user 메시지를 이용한 기본 요청\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYhhibXZSGTF",
    "outputId": "3d276aa8-39a2-4b85-b9aa-5bb43729783b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain은 똑똑한 앱을 쉽게 만드는 도구 모음이에요. 쉽게 말하면, 큰 언어 모델(예: 챗봇의 뇌)을 이용해서 데이터를 읽고, 도구를 부르고, 기억해 두고, 여러 단계를 순서대로 처리하는 프로그램을 잘 만들 수 있게 도와주는 라이브러리예요.\\n\\n주요 아이디어를 아주 간단히 비유로 보면:\\n- 프롬프트: 로봇에게 무엇을 하라고 지시하는 문구예요.\\n- 체인(연쇄): 여러 단계가 차례로 이어지도록 하는 작업 흐름이에요.\\n- 에이전트: 상황에 맞게 다음에 무엇을 할지 스스로 결정하는 똑똑한 아이예요.\\n- 도구/데이터 소스: 계산기, API, 문서, 데이터베이스 같은 필요한 도구와 자료예요.\\n\\n쉽게 말해, LangChain은 “로봇이 생각하고, 기억하고, 자료를 찾아보고, 필요하면 API 같은 도구도 쓰면서 일을 차근차근 해내도록” 만들어주는 코드 상자예요.\\n\\n누가 쓰나요? 개발자들이 파이썬이나 자바스크립트/타입스크립트로 사용해요.\\n\\n간단한 예: 내부 자료를 찾아 질문에 답하고, 필요하면 캘린더에 일정도 잡아주는 챗봇을 만들고 싶다면, LangChain이 그 흐름을 쉽고 잘 정리해 주는 식이에요.\\n\\n혹시 어떤 용도나 예제로 궁금한 게 있나요? 구체적으로 알려주시면 그에 맞춰 더 쉽게 설명해 드릴게요.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1397, 'prompt_tokens': 42, 'total_tokens': 1439, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1024, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CMkE8c5rdzjq9U40jSjaqn0fd6WuS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e36f9a4b-52b1-4285-9354-d4a65c67101a-0', usage_metadata={'input_tokens': 42, 'output_tokens': 1397, 'total_tokens': 1439, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1024}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "AK7Xe8wnSQGa",
    "outputId": "c314481f-1ecb-4337-bf3c-08c274701b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 똑똑한 앱을 쉽게 만드는 도구 모음이에요. 쉽게 말하면, 큰 언어 모델(예: 챗봇의 뇌)을 이용해서 데이터를 읽고, 도구를 부르고, 기억해 두고, 여러 단계를 순서대로 처리하는 프로그램을 잘 만들 수 있게 도와주는 라이브러리예요.\n",
      "\n",
      "주요 아이디어를 아주 간단히 비유로 보면:\n",
      "- 프롬프트: 로봇에게 무엇을 하라고 지시하는 문구예요.\n",
      "- 체인(연쇄): 여러 단계가 차례로 이어지도록 하는 작업 흐름이에요.\n",
      "- 에이전트: 상황에 맞게 다음에 무엇을 할지 스스로 결정하는 똑똑한 아이예요.\n",
      "- 도구/데이터 소스: 계산기, API, 문서, 데이터베이스 같은 필요한 도구와 자료예요.\n",
      "\n",
      "쉽게 말해, LangChain은 “로봇이 생각하고, 기억하고, 자료를 찾아보고, 필요하면 API 같은 도구도 쓰면서 일을 차근차근 해내도록” 만들어주는 코드 상자예요.\n",
      "\n",
      "누가 쓰나요? 개발자들이 파이썬이나 자바스크립트/타입스크립트로 사용해요.\n",
      "\n",
      "간단한 예: 내부 자료를 찾아 질문에 답하고, 필요하면 캘린더에 일정도 잡아주는 챗봇을 만들고 싶다면, LangChain이 그 흐름을 쉽고 잘 정리해 주는 식이에요.\n",
      "\n",
      "혹시 어떤 용도나 예제로 궁금한 게 있나요? 구체적으로 알려주시면 그에 맞춰 더 쉽게 설명해 드릴게요.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fi6_NvW3MaE"
   },
   "source": [
    "##  1. 프롬프트 템플릿과 로더를 사용하여 체인 구성  \n",
    "\n",
    "- LLM에서의 Chain 은 Data Processing 에서의 Pipeline 과 유사한 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G447ezl5X5ye"
   },
   "source": [
    "### 체인의 구성 요소 - Runnables\n",
    "- 프롬프트 : LLM의 응답을 안내하는 템플릿  \n",
    "- LLM 또는 채팅 모델 : 프롬프트에 따라 응답을 생성하는 엔진  \n",
    "- 출력 파서 : LLM의 출력을 파싱하는 도구  \n",
    "- 도구 : LLM이 API에서 추가 정보를 추출하거나 코드를 실행하여 LLM을 에이전트로 전환할 수 있게 해주는 확장 기능  \n",
    "- 일반 함수 : 서로 연결될 수 있는 추가적인 일반 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yt1j9kXR1h53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='\\n    당신은 AI 주제를 설명하는 데 도움이 되는 어시스턴트입니다. 다음 입력이 주어지면:\\n    {topic}\\n    주어진 주제에 대한 설명을 제공하세요.\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# simple prompt template 생성\n",
    "# {topic} 변수에서 사용자의 query 가 대체된다.\n",
    "prompt_template = \"\"\"\n",
    "    당신은 AI 주제를 설명하는 데 도움이 되는 어시스턴트입니다. 다음 입력이 주어지면:\n",
    "    {topic}\n",
    "    주어진 주제에 대한 설명을 제공하세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt template 을 이용하여 prompt 생성 - new style\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "0vikrfE53yDk",
    "outputId": "adf40012-9025-4a1a-a591-da9fd5b4883c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 대형 언어 모델(Large Language Model, LLM)을 활용한 애플리케이션을 쉽게 만들 수 있게 해주는 오픈소스 프레임워크예요. 프롬프트를 여러 단계로 연결하는 “체인(Chain)”과 외부 도구를 사용해 LLM의 능력을 확장하는 “에이전트(Agent)”를 중심으로 구성된 것이 특징입니다.\n",
      "\n",
      "주요 아이디어\n",
      "- LLM과 도구를 연결해 단일 프롬프트만으로 해결되지 않는 복잡한 작업을 처리할 수 있습니다.\n",
      "- 프롬프트 템플릿, 체인, 도구, 메모리(대화 맥락 저장) 등을 한 곳에서 구성하고 재사용할 수 있습니다.\n",
      "- Python과 TypeScript/JavaScript 환경에서 사용할 수 있습니다.\n",
      "\n",
      "주요 구성 요소\n",
      "- LLM: OpenAI, Cohere, HuggingFace 등 다양한 공급자의 모델을 래핑해 사용합니다.\n",
      "- PromptTemplates: 입력 형식을 미리 정의한 템플릿으로 프롬프트를 구성합니다.\n",
      "- Chains: LLMChain, SequentialChain 등으로 여러 프롬프트와 로직을 순차적으로 연결합니다.\n",
      "- Tools: 웹 검색, 계산기, 데이터베이스 질의, 파일 시스템 접근 등 LLM이 스스로 수행하기 어렵던 작업을 수행하는 도구들.\n",
      "- Agents: 도구를 무엇을 언제 쓸지 결정하는 의사결정 로직으로, ReAct 같은 전략으로 동작하는 경우가 많습니다.\n",
      "- Memory: 대화 맥락이나 요약 정보를 저장해 다음 상호작용에 활용합니다.\n",
      "- Vector Stores / Retrieval: 임베딩을 저장하고 필요 시 관련 문서를 검색해 참고합니다.\n",
      "- LangChain 런타임: Python용 LangChain, TypeScript용 LangChain.js 등 언어별 라이브러리.\n",
      "\n",
      "작동 원리의 간단한 흐름\n",
      "- 사용자의 입력이 프롬프트 템플릿으로 변환되고, LLM에 전달됩니다.\n",
      "- 필요하면 Tools를 호출해 외부 데이터를 얻거나 계산 등을 수행합니다.\n",
      "- LLM의 출력과 도출된 정보를 다시 프롬프트에 입력해 추가적으로 응답을 생성합니다.\n",
      "- 에이전트의 경우 이 사이클을 반복하며 목표를 달성할 때까지 도구를 사용합니다.\n",
      "\n",
      "주요 사용 사례\n",
      "- 대화형 챗봇에 외부 도구 통합(검색, 예약, 계산 등)하기\n",
      "- 문서 QA, 요약, 정보 추출 파이프라인 구축\n",
      "- 코드 보조/자동화 도구, 데이터 분석 보조\n",
      "- 여러 데이터 소스에서 정보를 모아 응답 생성하기\n",
      "\n",
      "장점과 한계\n",
      "- 장점: 모듈화된 구성 요소로 재사용이 쉽고, 도구와의 연동으로 LLM의 한계를 보완하며, 대화형 메모리나 문서 검색 등 기능 확장이 용이합니다.\n",
      "- 한계: 시스템 복잡도가 증가하고, 프롬프트 설계와 도구 연동에 따른 비용과 유지 관리가 필요합니다. 안전성과 프라이버시도 고려해야 합니다.\n",
      "\n",
      "시작하기\n",
      "- 설치 예시(파이썬): pip install langchain\n",
      "- 간단한 사용 흐름: LLMChain으로 프롬프트를 만들어 실행하거나, 에이전트를 구성해 도구를 차례로 호출하도록 만듭니다.\n",
      "- 공식 문서와 예제 코드를 참고하면 실제로 어떤 구성 요소를 어떻게 연결하는지 쉽게 배울 수 있습니다.\n",
      "\n",
      "요약하면, LangChain은 LLM을 “다룰 수 있는 도구”로 확장해주는 프레임워크로, 복잡한 의사결정과 다수의 데이터/도구를 조합하는 애플리케이션을 빠르게 구축하도록 돕습니다.\n"
     ]
    }
   ],
   "source": [
    "# pipe operator \"|\"\" 를 이용하여 하나 이상의 chain 을 결합\n",
    "chain = prompt | model\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LangChain이 뭐예요?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Runnable과 LangChain 표현 언어  \n",
    "\n",
    "- LCEL(LangChain Expression Language) 는 Runnables 를 chain 으로 구성하는 방법\n",
    "\n",
    "LCEL은 기본 구성 요소에서 복잡한 체인을 구축하는 것을 간소화합니다. 파이프 연산자(|)를 사용하여 다양한 구성 요소를 체인으로 연결하고 한 요소에서 다음 요소로 출력을 공급합니다. 이런 방식으로 구성된 체인의 간단한 예로는 모델과 출력 파서가 결합된 프롬프트가 있습니다.  이러한 구성 요소들을 runnables 라고 부릅니다.  \n",
    "\n",
    "`chain=prompt | model | output_parser`\n",
    "\n",
    "- Chain 구성  \n",
    "    - 체인에 대한 입력(일반적으로 사전)\n",
    "    - 입력은 프롬프트로 전송됩니다.\n",
    "    - 프롬프트 값은 LLM 또는 채팅 모델로 전송됩니다.\n",
    "    - Chatmodel이 채팅 메시지를 반환합니다.\n",
    "    - 파서는 채팅 메시지에서 문자열을 추출합니다.\n",
    "    - 문자열은 체인의 출력입니다\n",
    "\n",
    "- LangChain의 runnable 객체들:\n",
    "\n",
    "    - RunnableSequence : 여러 runnable 구성 요소를 연결하여 각 구성 요소가 입력을 처리하고 출력을 다음 구성 요소에 전달\n",
    "    - RunnableLambda : Python의 호출 가능한 요소(함수 등)를 실행 가능한 구성 요소로 바꿔서 체인으로 통합\n",
    "    - RunnablePassthrough : 입력을 변경하지 않고 통과시키거나 출력에 추가 키를 추가. placeholder 역할을 하거나 시퀀스에 유연하게 통합할 수 있다.\n",
    "    - RunnableParallel : 여러 개의 실행 파일을 동시에 실행하여 두 개의 체인이 동일한 입력에서 실행되지만 다른 출력을 반환하는 분기를 허용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3s0DaSYO_W0Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    당신은 AI 개념을 요약하는 유용한 조수입니다.\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    당신은 AI 개념을 요약하는 유용한 조수입니다.\n",
    "    {context}\n",
    "    context를 한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(prompt_template)\n",
    "summarize_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InN3aD1qGwmD"
   },
   "source": [
    "###  \"|\" 연산자를 이용하여 Runnable Sequence chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0cahXhesl-E",
    "outputId": "eed661f4-9190-4da5-f8df-0139d0df7b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = summarize_prompt | model | output_parser\n",
    "\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "WpJAZmpOsjHA",
    "outputId": "147f9a53-6d84-4d3c-cae6-4322635e23f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다음은 LangChain에 대한 간략한 설명과 한국어로의 요약입니다.\\n\\n- LangChain이란\\n  - LLM(대형 언어 모델) 기반 애플리케이션을 쉽게 만들고 운영하기 위한 프레임워크/생태계입니다.\\n  - 프롬프트를 체인처럼 연결하는 체인(Chains), 상황에 따라 동적으로 도구를 사용할 수 있게 해주는 에이전트(Agents), 대화나 작업 흐름에서 상태를 기억하는 메모리(Memory), 외부 소스나 도구를 연결하는 도구(Tools) 등 다양한 구성 요소를 제공합니다.\\n  - 벡터 저장소, 문서 로더, 임베딩(임베딩 모델), 검색기(Retriever) 같은 기능을 통해 데이터 인입과 검색 기반의 응답을 쉽게 구현할 수 있습니다.\\n  - Python용과 JavaScript/TypeScript용 두 가지 구현체가 있어, 서버 사이드나 웹 애플리케이션에 맞춰 사용할 수 있습니다.\\n\\n- 주요 구성 요소\\n  - LLM 래퍼/프로바이더: OpenAI, Cohere, HuggingFace 등 다양한 LLM을 추상화하여 동일한 인터페이스로 조작.\\n  - Prompt Templates: PromptTemplate, ChatPromptTemplate 등을 사용해 입력 형식을 표준화하고 재사용 가능한 프롬프트를 만듭니다.\\n  - Chains: LLMChain, 데이터 처리를 순차적으로 연결하는 여러 종류의 체인(순차적, 조건부, 순환 등)을 제공합니다.\\n  - Agents와 Tools: 에이전트가 상황에 따라 도구(툴)를 호출하고, 필요 시 외부 API나 데이터베이스를 사용할 수 있도록 합니다.\\n  - Memory: ConversationalMemory, SummaryMemory 등 대화 맥락이나 장기 기억을 저장하고 활용합니다.\\n  - Vector Stores와 Retrievers: FAISS, Pinecone, Weaviate 등 벡터 저장소를 이용해 문서를 임베딩하고 검색합니다.\\n  - Document Loaders: PDF, Word, HTML, 웹 스크랩 등 다양한 소스에서 문서를 불러옵니다.\\n  - 콜백/로깅과 Persistence: 실행 흐름의 로깅, 결과 파싱, 결과 저장 등을 지원합니다.\\n  - 다중 런타임/환경 지원: LangChain.py(파이썬)와 LangChain.js(자바스크립트/타입스크립트)로 다양한 스택에 적용 가능합니다.\\n\\n- 작동 방식의 간단한 흐름\\n  - 데이터 소스(문서, 웹, 데이터베이스 등)에서 정보를 불러오고 임베딩하여 벡터 저장소에 저장합니다.\\n  - 사용자가 질문을 주면, 체인이나 에이전트가 LLM에 프롬프트를 구성해 질의를 보내고 필요하면 도구를 호출합니다.\\n  - 도구 호출 결과를 다시 LLM에 전달해 최종 응답을 만듭니다.\\n  - 대화 맥락이나 기억이 필요한 경우 Memory를 통해 이전 정보를 활용합니다.\\n\\n- 일반적인 사용 사례\\n  - 지식 기반 QA: 대규모 문서를 바탕으로 한 질문 응답\\n  - 챗봇/대화형 에이전트: 도구를 호출해 작업 자동화(예: 일정 관리, 데이터 조회)\\n  - 데이터 분석 보조: 코드 실행 없이도 데이터 요약, 패턴 발견 등\\n  - 코드 보조/리포트 작성: LLM과 도구를 연결해 자동화된 문서 작성\\n  - AI 앱 프로토타이핑: 빠르게 MVP를 구축하고 테스트\\n\\n- 설치와 시작 방법(간단)\\n  - 파이썬: pip install langchain\\n  - 자바스크립트/타입스크립트: npm install langchain\\n  - 간단한 흐름 예시로는 LLMChain과 PromptTemplate으로 질문에 대한 답변 생성, 또는 에이전트가 도구를 호출하도록 구성하는 방식이 일반적입니다.\\n\\n- 주의점\\n  - 비용과 지연(Latency): 외부 API를 많이 호출할수록 비용과 응답 지연이 증가할 수 있습니다.\\n  - 복잡도 관리: 강력한 기능일수록 프로젝트 구조를 잘 관리해야 유지보수에 유리합니다.\\n  - 보안/프라이버시: 외부 도구 호출 시 데이터 처리 정책과 보안 상황을 점검해야 합니다.\\n  - 의존성/버전 이슈: 다양한 컴포넌트가 서로 다른 버전에 맞춰 동작하므로 버전 관리가 필요합니다.\\n\\n- 한국어 요약(컨텍스트 요약)\\n  LangChain은 LLM 기반 애플리케이션 개발을 빠르게 만들고 운영하기 위한 프레임워크다. 프롬프트를 체인처럼 연결하는 체인(Chains), 상황에 따라 도구를 활용하는 에이전트(Agents), 대화나 작업 맥락을 기억하는 메모리(Memory), 외부 소스와 도구를 연결하는 도구/툴(Tools) 등의 핵심 구성요소를 제공한다. 또한 벡터 저장소와 문서 로더를 통해 데이터 인입과 검색 기반의 응답을 손쉽게 구현할 수 있으며, 파이썬과 자바스크립트/타입스크립트 양쪽에서 사용할 수 있다. 이를 통해 지식 기반 QA, 대화형 에이전트, 데이터 분석 보조 등 다양한 LLM 애플리케이션을 빠르게 프로토타이핑하고 운영할 수 있다.\\n\\n혹시 특정 용도(예: 챗봇 구축, 문서 기반 QA, API 연동 자동화)나 언어(파이썬 vs 자바스크립트)에서 LangChain을 어떻게 활용하면 좋은지에 대한 구체적 예시를 원하시면 말씀해 주세요. 해당 목적에 맞춘 간단한 설계 예시나 코드 구조도 같이 설명해 드리겠습니다.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_VLKGZ1HO7d"
   },
   "source": [
    "### RunnableLambda 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oI9MZ0sasxQf",
    "outputId": "c5356d5c-6811-43af-9bfb-31fe88e2638e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambda를 사용하여 Python 함수를 체인에 삽입합니다.\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summarize_chain = summarize_prompt | model | output_parser\n",
    "print(type(summarize_chain))\n",
    "\n",
    "# 사용자 정의 람다 함수를 정의하고 이를 RunnableLambda에 래핑합니다.\n",
    "length_lambda = RunnableLambda(lambda summary: f\"요약된 길이: {len(summary)} 글자\")\n",
    "print(type(length_lambda))\n",
    "\n",
    "lambda_chain = summarize_chain | length_lambda\n",
    "print(type(lambda_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TAa1W6iVHLnq",
    "outputId": "5b181616-a4f7-49a1-b807-9795fc389b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요약된 길이: 1777 글자'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhIbYeOgIYyi",
    "outputId": "6ee972ca-751b-4b86-a8cd-fefe6fbe8c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] input_types={} partial_variables={} template='\\n    당신은 AI 개념을 요약하는 유용한 조수입니다.\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n'\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10a32bcd0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10a440510> root_client=<openai.OpenAI object at 0x109bac850> root_async_client=<openai.AsyncOpenAI object at 0x109bac3d0> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "\n",
      "RunnableLambda(lambda summary: f'요약된 길이: {len(summary)} 글자')\n"
     ]
    }
   ],
   "source": [
    "for step in lambda_chain.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
